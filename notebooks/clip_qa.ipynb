{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json \n",
    "import sys \n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import loaders\n",
    "from utils import scores as sc\n",
    "from utils import evaluation as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Set Logger\n",
    "logger = logging.getLogger('notebook_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one example\n",
    "# TODO: Add VSNR for cosine similarity\n",
    "data_loader = loaders.DataLoader(data_source = \"qa\", logger=logger)\n",
    "df_table = data_loader.load_dialogue_df()\n",
    "\n",
    "#k = 5\n",
    "#data_loader.showing_example(k)\n",
    "#data_loader.show_clip_similarity(k, df_table, model, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define OOD Categories below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_category = ['vehicle']\n",
    "ind_category = [x for x in data_loader.supercategories if x not in ood_category]\n",
    "df_table['OOD'] = df_table['supercategory'].apply(lambda x: 1 if any(item in x for item in ind_category) else 0)\n",
    "df_table['OOD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CLIP features for images and dialogues with Model CLIP ViT-B32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_loader.data_source == \"real\":\n",
    "    ## Dialogue Processing\n",
    "    dialogue_clip = np.load(f'{data_loader.data_dir}/CLIP/mmd_dialogs_truncate/mmd_clip_dialog_features.npy')\n",
    "    df_table['dialogue_clip'] = list(dialogue_clip)\n",
    "\n",
    "    ## Image Processing\n",
    "    image_clip = np.load(f'{data_loader.data_dir}/CLIP/mmd_imgs/mmd_clip_img_features.npy')\n",
    "    image_annotation = pd.read_json(f'{data_loader.data_dir}/CLIP/mmd_imgs/mmd_imgs_filenames.json')\n",
    "    image_annotation = image_annotation.rename(columns={0:\"img_file\"}).join(pd.DataFrame(pd.DataFrame(image_clip.tolist()).apply(np.array, axis=1)))\n",
    "    image_annotation.rename(columns={0:\"image_clip\"}, inplace=True)\n",
    "    df_table = df_table.merge(image_annotation, on='img_file', how='left')\n",
    "\n",
    "elif data_loader.data_source == \"qa\":\n",
    "    ## Dialogue Processing\n",
    "    dialogue_clip = np.load(f'{data_loader.data_dir}/CLIP/qa_dialogs_truncate/qa_clip_dialog_features.npy')\n",
    "    df_table['dialogue_clip'] = list(dialogue_clip)\n",
    "\n",
    "    ## Image Processing\n",
    "    df_table['image_file'] = df_table['image_id'].astype('str') + '.jpg'\n",
    "    image_clip = np.load(f'{data_loader.data_dir}/CLIP/qa_imgs/qa_clip_img_features.npy')\n",
    "    image_annotation = pd.read_json(f'{data_loader.data_dir}/CLIP/qa_imgs/all_img_names.json')\n",
    "    image_annotation = image_annotation.rename(columns={0:\"image_file\"})\n",
    "    image_annotation['image_clip'] = list(image_clip)\n",
    "    df_table = df_table.merge(image_annotation, on='image_file', how='left') \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dialogue_df_segment = pd.read_json(f'{data_loader.data_dir}/CLIP/mmd_dialogs_segment/new_mmd_expand_dialog.json')\n",
    "#dialogue_clip_segment = np.load(f'{data_loader.data_dir}/CLIP/mmd_dialogs_segment/new_mmd_dialog_features.npy')\n",
    "#dialogue_df_segment['dialogue_clip'] = list(dialogue_clip_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=ind_category)\n",
    "df_table['encoded_label'] = list(mlb.fit_transform(df_table['supercategory']))\n",
    "encoded_df = pd.DataFrame(df_table['encoded_label'].tolist(), columns=ind_category)\n",
    "df_table = pd.concat([df_table, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#if data_loader.data_source == \"qa\":\n",
    "#    df_table['image_id'] = df_table['image_id'].apply(lambda x: f\"COCO_train2014_{int(x):012d}\")\n",
    "\n",
    "categories_clip = {}\n",
    "for categories in ind_category:\n",
    "    text = 'Category ' + categories\n",
    "    text_tokens = clip.tokenize([text]).to(device)  \n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens).flatten().cpu().numpy()\n",
    "        categories_clip[categories] = text_features\n",
    "\n",
    "def calculate_similarity_score(row, type = \"image\"):\n",
    "    if type == \"image\":\n",
    "        column = 'image_clip'\n",
    "    elif type == \"dialogue\":\n",
    "        column = 'dialogue_clip'\n",
    "\n",
    "    cosine_sim = 0\n",
    "    cosine_sim_max = 0\n",
    "    for categories in ind_category:\n",
    "        text_features = categories_clip[categories]\n",
    "        cosine_sim_current = np.dot(text_features, row[column]) / (np.linalg.norm(text_features) * np.linalg.norm(row[column]))\n",
    "        cosine_sim += cosine_sim_current\n",
    "        cosine_sim_max = max(cosine_sim_max, cosine_sim_current)\n",
    "\n",
    "\n",
    "    return cosine_sim, cosine_sim_max\n",
    "\n",
    "df_table['image_score'], df_table['image_score_max'] = zip(*df_table.progress_apply(calculate_similarity_score, axis=1))\n",
    "#dialogue_df_segment['dialogue_score'], dialogue_df_segment['dialogue_score_max'] = zip(*dialogue_df_segment.progress_apply(calculate_similarity_score, type = \"dialogue\", axis=1))\n",
    "df_table['dialogue_score'], df_table['dialogue_score_max'] = zip(*df_table.progress_apply(calculate_similarity_score, axis=1, args=('dialogue',)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_table['dialogue_score_segment'] = pd.DataFrame(dialogue_df_segment.groupby('index')['dialogue_score'].mean())['dialogue_score'].values\n",
    "#df_table['dialogue_score_segment_max'] = pd.DataFrame(dialogue_df_segment.groupby('index')['dialogue_score_max'].mean())['dialogue_score_max'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\"\n",
    "})\n",
    "plt.rcParams['figure.dpi'] = 150 \n",
    "# Assuming df_table is preloaded with the appropriate columns\n",
    "# Simplifying the code and plotting all histograms in a 2x2 layout\n",
    "\n",
    "# Create figure and axes for a 2x2 grid\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Data for histograms\n",
    "columns = ['image_score', 'image_score_max', 'dialogue_score', 'dialogue_score_max'] #'dialogue_score_segment', 'dialogue_score_segment_max']\n",
    "titles = ['Image Scores Distribution', 'Image Max Scores Distribution',\n",
    "          'Dialogue Sum Scores Distribution', 'Dialogue Max Scores Distribution']\n",
    "         #'Dialogue Segment Sum Scores Distribution', 'Dialogue Segment Max Scores Distribution']\n",
    "x_labels = ['Image Score', 'Image Max Score', 'Dialogue Score', 'Dialogue Max Score'] # 'Dialogue Segment Score', 'Dialogue Segment Max Score']\n",
    "\n",
    "# Loop through to plot each histogram in its subplot\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    ood_scores = df_table[df_table['OOD'] == 0][columns[i]]\n",
    "    non_ood_scores = df_table[df_table['OOD'] == 1][columns[i]]\n",
    "    sns.histplot(non_ood_scores, bins=80, alpha=0.5, label='ID', kde=True, color='blue', ax=ax, stat=\"density\")\n",
    "    sns.histplot(ood_scores, bins=80, alpha=0.5, label='OOD', kde=True, color='red', ax=ax, stat=\"density\")\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel(x_labels[i])\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text_similarity(row):\n",
    "    a = row['dialogue_clip']\n",
    "    b = row['image_clip']\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "df_table['image_text_similarity'] = df_table.apply(image_text_similarity, axis=1)\n",
    "df_table['overall_simialrity'] = df_table['image_text_similarity'] * (df_table['image_score_max'] + 0.01*df_table['dialogue_score_max'])\n",
    "df_table['overall_simialrity_sum'] = df_table['image_text_similarity'] * (df_table['image_score'] + 0.01*df_table['dialogue_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image Max:', ev.fpr_evaluation(df_table['OOD'].values, df_table['image_score_max'].values, 0.95))\n",
    "print('Image Sum:', ev.fpr_evaluation(df_table['OOD'].values, df_table['image_score'].values, 0.95))\n",
    "print('Dialogue Max:', ev.fpr_evaluation(df_table['OOD'].values, df_table['dialogue_score_max'].values, 0.95))\n",
    "print('Dialogue Sum:', ev.fpr_evaluation(df_table['OOD'].values, df_table['dialogue_score'].values, 0.95))\n",
    "#print('Dialogue Segment Max:', ev.fpr_evaluation(df_table['OOD'].values, -df_table['dialogue_score_segment_max'].values, 0.95))\n",
    "#print('Dialogue Segment Sum:', ev.fpr_evaluation(df_table['OOD'].values, -df_table['dialogue_score_segment'].values, 0.95))\n",
    "print(\"Overall Max:\", ev.fpr_evaluation(df_table['OOD'].values, df_table['overall_simialrity'].values, 0.95))\n",
    "print(\"Overall Sum:\", ev.fpr_evaluation(df_table['OOD'].values, df_table['overall_simialrity_sum'].values, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.DNN import model \n",
    "\n",
    "image_model_loader = model.model_loader(logger=logger,\n",
    "                                  num_epochs=30,\n",
    "                                  learning_rate=0.001,\n",
    "                                  proportion = 0.8)\n",
    "\n",
    "dialogue_model_loader = model.model_loader(logger=logger,\n",
    "                                    num_epochs=30,\n",
    "                                    learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_ind_train, \n",
    "    df_test, \n",
    "    X_train_image, \n",
    "    X_test_image, \n",
    "    X_train_dialogue, \n",
    "    X_test_dialogue, \n",
    "    Y_train, \n",
    "    Y_test) = image_model_loader.create_dataset(data_loader, df_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model_loader.train_model(X_train_image, Y_train, X_test_image, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_model_loader.train_model(X_train_dialogue, Y_train, X_test_dialogue, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type_list = [\"mp\", \"energy\", \"maxlogits\", \"msp\"]\n",
    "\n",
    "for score_type in score_type_list:\n",
    "    image_score_sum, image_score_max = image_model_loader.evaluate_on_test(X_test_image, \n",
    "                                                        Y_test,\n",
    "                                                        score_type=score_type,\\\n",
    "                                                        return_score=True)\n",
    "    \n",
    "    dialogue_score_sum, dialogue_score_max = dialogue_model_loader.evaluate_on_test(X_test_dialogue, \n",
    "                                                        Y_test,\n",
    "                                                        score_type=score_type,\\\n",
    "                                                        return_score=True)\n",
    "    \n",
    "    df_test[f'{score_type}_sum_image'] = image_score_sum\n",
    "    df_test[f'{score_type}_max_image'] = image_score_max\n",
    "    df_test[f'{score_type}_sum_dialogue'] = dialogue_score_sum\n",
    "    df_test[f'{score_type}_max_dialogue'] = dialogue_score_max\n",
    "    df_test[f'{score_type}_overall_simialrity'] = df_test['image_text_similarity'] * (df_test[f'{score_type}_max_image'] + df_test[f'{score_type}_max_dialogue'])\n",
    "    if score_type == \"energy\":\n",
    "        df_test[f'{score_type}_overall_simialrity_sum'] = df_test['image_text_similarity'] * (df_test[f'{score_type}_sum_image'] + df_test[f'{score_type}_sum_dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type = \"energy\"\n",
    "type = \"sum\"\n",
    "fpr = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ood_scores = df_test[df_test['OOD'] == 0][f'{score_type}_{type}_image']\n",
    "non_ood_scores = df_test[df_test['OOD'] == 1][f'{score_type}_{type}_image']\n",
    "\n",
    "sns.histplot(non_ood_scores, bins=80, alpha=0.5, label='ID', kde=True, color='blue',stat=\"density\")\n",
    "sns.histplot(ood_scores, bins=80, alpha=0.5, label='OOD', kde=True, color='red', stat=\"density\")\n",
    "plt.title('Energy Distribution')\n",
    "plt.xlabel(f'Image {type.title()} Score')\n",
    "plt.ylabel('Probability Density')\n",
    "hist_id, bins_id = np.histogram(non_ood_scores, bins=80, density=True)\n",
    "cumulative_id = np.cumsum(hist_id * np.diff(bins_id))\n",
    "threshold_index_id = np.where(cumulative_id >= (1 - fpr/100))[0][0]\n",
    "threshold_value_id = bins_id[threshold_index_id]\n",
    "hist_ood, bins_ood = np.histogram(ood_scores, bins=80, density=True)\n",
    "cumulative_ood = np.cumsum(hist_ood * np.diff(bins_ood))\n",
    "threshold_index_ood = np.searchsorted(bins_ood, threshold_value_id) - 1\n",
    "cumulative_probability_ood = cumulative_ood[threshold_index_ood]\n",
    "plt.axvline(x=threshold_value_id, color='green', linestyle='--', label=f'{fpr}\\% Recall at {threshold_value_id:.2f}')\n",
    "plt.axvline(x=threshold_value_id, color='purple', linestyle=':', label=f'FPR{fpr} with {(1 - cumulative_probability_ood):.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ood_scores = df_test[df_test['OOD'] == 0][f'{score_type}_{type}_dialogue']\n",
    "non_ood_scores = df_test[df_test['OOD'] == 1][f'{score_type}_{type}_dialogue']\n",
    "\n",
    "sns.histplot(non_ood_scores, bins=80, alpha=0.5, label='ID', kde=True, color='blue',stat=\"density\")\n",
    "sns.histplot(ood_scores, bins=80, alpha=0.5, label='OOD', kde=True, color='red', stat=\"density\")\n",
    "plt.title('Energy Distribution')\n",
    "plt.xlabel(f'Dialogue {type.title()} Score')\n",
    "plt.ylabel('Probability Density')\n",
    "hist_id, bins_id = np.histogram(non_ood_scores, bins=80, density=True)\n",
    "cumulative_id = np.cumsum(hist_id * np.diff(bins_id))\n",
    "threshold_index_id = np.where(cumulative_id >= (1 - fpr/100))[0][0]\n",
    "threshold_value_id = bins_id[threshold_index_id]\n",
    "hist_ood, bins_ood = np.histogram(ood_scores, bins=80, density=True)\n",
    "cumulative_ood = np.cumsum(hist_ood * np.diff(bins_ood))\n",
    "threshold_index_ood = np.searchsorted(bins_ood, threshold_value_id) - 1\n",
    "cumulative_probability_ood = cumulative_ood[threshold_index_ood]\n",
    "plt.axvline(x=threshold_value_id, color='green', linestyle='--', label=f'{fpr}\\% Recall at {threshold_value_id:.2f}')\n",
    "plt.axvline(x=threshold_value_id, color='purple', linestyle=':', label=f'FPR{fpr} with {(1 - cumulative_probability_ood):.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ood_scores = df_test[df_test['OOD'] == 0][f'{score_type}_overall_simialrity_sum']\n",
    "non_ood_scores = df_test[df_test['OOD'] == 1][f'{score_type}_overall_simialrity_sum']\n",
    "sns.histplot(non_ood_scores, bins=80, alpha=0.5, label='ID', kde=True, color='blue',  stat=\"density\")\n",
    "sns.histplot(ood_scores, bins=80, alpha=0.5, label='OOD', kde=True, color='red',  stat=\"density\")\n",
    "plt.title('Energy Distribution')\n",
    "plt.xlabel(f'Overall {type.title()} Score')\n",
    "plt.ylabel('Probability Density')\n",
    "hist_id, bins_id = np.histogram(non_ood_scores, bins=80, density=True)\n",
    "cumulative_id = np.cumsum(hist_id * np.diff(bins_id))\n",
    "threshold_index_id = np.where(cumulative_id >= (1 - fpr/100))[0][0]\n",
    "threshold_value_id = bins_id[threshold_index_id]\n",
    "hist_ood, bins_ood = np.histogram(ood_scores, bins=80, density=True)\n",
    "cumulative_ood = np.cumsum(hist_ood * np.diff(bins_ood))\n",
    "threshold_index_ood = np.searchsorted(bins_ood, threshold_value_id) - 1\n",
    "cumulative_probability_ood = cumulative_ood[threshold_index_ood]\n",
    "plt.axvline(x=threshold_value_id, color='green', linestyle='--', label=f'{fpr}\\% Recall at {threshold_value_id:.2f}')\n",
    "plt.axvline(x=threshold_value_id, color='purple', linestyle=':', label=f'FPR{fpr} with {(1 - cumulative_probability_ood):.2f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "metrics = []\n",
    "values = []\n",
    "scores = [] \n",
    "\n",
    "def eval_dict(score):\n",
    "    return {\n",
    "        \"FPR\": lambda x: ev.fpr_evaluation(x['OOD'].values, x[score].values, 0.95),\n",
    "        \"AUROC\": lambda x: ev.auroc_evaluation(x['OOD'].values, x[score].values),\n",
    "        \"AUPR\": lambda x: ev.aupr_evaluation(x['OOD'].values, x[score].values)\n",
    "    }\n",
    "\n",
    "# Define the metrics and corresponding functions\n",
    "metric_functions = {\n",
    "    \"Max Cosine\": {\n",
    "        \"Image\": eval_dict('image_score_max'),\n",
    "        \"Dialogue\": eval_dict('dialogue_score_max'),\n",
    "        \"Overall\": eval_dict('overall_simialrity')\n",
    "    },\n",
    "    \"Sum Cosine\": {\n",
    "        \"Image\": eval_dict('image_score'),\n",
    "        \"Dialogue\": eval_dict('dialogue_score'),\n",
    "        \"Overall\": eval_dict('overall_simialrity_sum')\n",
    "    },\n",
    "    \"Energy Sum\": {\n",
    "        \"Image\": eval_dict('energy_sum_image'),\n",
    "        \"Dialogue\": eval_dict('energy_sum_dialogue'),\n",
    "        \"Overall\": eval_dict('energy_overall_simialrity_sum')\n",
    "    },\n",
    "    \"Energy Max\": {\n",
    "        \"Image\": eval_dict('energy_max_image'),\n",
    "        \"Dialogue\": eval_dict('energy_max_dialogue'),\n",
    "        \"Overall\": eval_dict('energy_overall_simialrity')\n",
    "    },\n",
    "    \"MSP\": {\n",
    "        \"Image\": eval_dict('msp_max_image'),\n",
    "        \"Dialogue\": eval_dict('msp_max_dialogue'),\n",
    "        \"Overall\": eval_dict('msp_overall_simialrity')\n",
    "    },\n",
    "    \"Max Prob\": {\n",
    "        \"Image\": eval_dict('mp_max_image'),\n",
    "        \"Dialogue\": eval_dict('mp_max_dialogue'),\n",
    "        \"Overall\": eval_dict('mp_overall_simialrity')\n",
    "    },\n",
    "    \"Max Logits\": {\n",
    "        \"Image\": eval_dict('maxlogits_max_image'),\n",
    "        \"Dialogue\": eval_dict('maxlogits_max_dialogue'),\n",
    "        \"Overall\": eval_dict('maxlogits_overall_simialrity')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each metric and calculate values\n",
    "for score, items in metric_functions.items():\n",
    "    scores.extend([score] * len(items) * 3)\n",
    "    for metric, funcs in items.items():\n",
    "        metrics.extend([metric] * len(funcs))\n",
    "        values.extend([func(df_test) for func in funcs.values()])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"Metric\": metrics, \"Value\": values, \"Score\": scores})\n",
    "df['Value'] = df['Value'].apply(lambda x: round(x, 3))\n",
    "#df_grouped = df.groupby('Metric')['Value'].apply(list).reset_index()\n",
    "result = df.groupby(['Metric', 'Score'])['Value'].agg(list).unstack().transpose()\n",
    "result[['Image', 'Dialogue', 'Overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
