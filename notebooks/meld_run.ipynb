{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json \n",
    "import sys \n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "import loaders\n",
    "from utils import scores as sc\n",
    "from utils import evaluation as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Set Logger\n",
    "logger = logging.getLogger('notebook_logger')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing one example\n",
    "model_type = 'clip'\n",
    "data_loader = loaders.DataLoader(data_source = \"meld\", model_type=model_type, logger=logger)\n",
    "df_table = data_loader.load_annotations_df()\n",
    "df_table = df_table.rename(columns={0: \"image_name\", 1: \"utterance\", 2: \"index\", 3: \"utterance_id\", 4: \"supercategories\", 5: \"sentiment\"})\n",
    "#k = 5\n",
    "#data_loader.showing_example(k)\n",
    "#data_loader.show_clip_similarity(k, df_table, model, preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define OOD Categories below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv_model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "new_supercategories = ['surprise', \n",
    "                   'anger', \n",
    "                   'neutral',\n",
    "                   'joy',\n",
    "                   'sadness',\n",
    "                   'disgust',\n",
    "                   'fear']\n",
    "lemmatized_supercategories = [lemmatizer.lemmatize(word) for word in new_supercategories]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_list = []\n",
    "for category_a in new_supercategories:\n",
    "    cosine_sim_current = 0\n",
    "    for category_b in new_supercategories:\n",
    "        text_tokens_a = wv_model[category_a]\n",
    "        text_tokens_b = wv_model[category_b]\n",
    "        cosine_sim_current += 1 - np.dot(text_tokens_a, text_tokens_b) / (np.linalg.norm(text_tokens_a) * np.linalg.norm(text_tokens_b))\n",
    "    print(f\"Cosine similarity for {category_a} is: {cosine_sim_current - 1}\")\n",
    "    cosine_similarity_list.append(cosine_sim_current - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn_list = []\n",
    "for category_a in lemmatized_supercategories:\n",
    "    sim_current = 0\n",
    "    for category_b in lemmatized_supercategories:\n",
    "        similarity = 1 - wn.synset(f\"{category_a}.n.01\").path_similarity(wn.synset(f\"{category_b}.n.01\"))\n",
    "        #logger.info(f\"Similarity between {category_a} and {category_b} is: {similarity}\")\n",
    "        sim_current += similarity\n",
    "    logger.info(f\"Sum similarity for {category_a} is: {sim_current - 1}\")\n",
    "    wn_list.append(sim_current - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_average = {}\n",
    "for i in range(len(new_supercategories)):\n",
    "    #logger.info(f\"Category: {new_supercategories[i]} - Cosine Similarity: {cosine_similarity_list[i]} - WordNet Similarity: {wn_list[i]}\")\n",
    "    average = (cosine_similarity_list[i] + wn_list[i]) / 2\n",
    "    logger.info(f\"Category: {new_supercategories[i]} Average: {average}\")\n",
    "    final_average[new_supercategories[i]] = average\n",
    "\n",
    "df = pd.DataFrame(list(final_average.items()), columns=['Key', 'Value'])\n",
    "df.sort_values(by='Value', ascending=False, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_category = ['anger']\n",
    "ind_category = [x for x in data_loader.supercategories if x not in ood_category]\n",
    "df_table['OOD'] = df_table['supercategories'].apply(lambda x: 0 if any(item in x for item in ood_category) else 1)\n",
    "df_table['OOD'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CLIP features for images and dialogues with Model CLIP ViT-B32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_loader.data_source == \"real\":\n",
    "    ## Dialogue Processing\n",
    "    dialogue_clip = np.load(f'{data_loader.data_dir}/CLIP/mmd_dialogs_truncate/mmd_clip_dialog_features.npy')\n",
    "    df_table['dialogue_clip'] = list(dialogue_clip)\n",
    "\n",
    "    ## Image Processing\n",
    "    image_clip = np.load(f'{data_loader.data_dir}/CLIP/mmd_imgs/mmd_clip_img_features.npy')\n",
    "    image_annotation = pd.read_json(f'{data_loader.data_dir}/CLIP/mmd_imgs/mmd_imgs_filenames.json')\n",
    "    image_annotation = image_annotation.rename(columns={0:\"img_file\"}).join(pd.DataFrame(pd.DataFrame(image_clip.tolist()).apply(np.array, axis=1)))\n",
    "    image_annotation.rename(columns={0:\"image_clip\"}, inplace=True)\n",
    "    df_table = df_table.merge(image_annotation, on='img_file', how='left')\n",
    "\n",
    "elif data_loader.data_source == \"qa\":\n",
    "    ## Dialogue Processing\n",
    "    dialogue_clip = np.load(f'{data_loader.data_dir}/CLIP/qa_dialogs_truncate/qa_clip_dialog_features.npy')\n",
    "    df_table['dialogue_clip'] = list(dialogue_clip)\n",
    "\n",
    "    ## Image Processing\n",
    "    df_table['image_file'] = df_table['image_id'].astype('str') + '.jpg'\n",
    "    image_clip = np.load(f'{data_loader.data_dir}/CLIP/QA_imgs/qa_clip_img_features.npy')\n",
    "    image_annotation = pd.read_json(f'{data_loader.data_dir}/CLIP/QA_imgs/all_img_names.json')\n",
    "    image_annotation = image_annotation.rename(columns={0:\"image_file\"})\n",
    "    image_annotation['image_clip'] = list(image_clip)\n",
    "    df_table = df_table.merge(image_annotation, on='image_file', how='left') \n",
    "\n",
    "elif data_loader.data_source == \"meld\":\n",
    "    df_dialogue = data_loader.load_dialogue_df()\n",
    "    df_image = data_loader.load_image_df()\n",
    "    df_table[f\"dialogue_{model_type}\"] = list(df_dialogue)\n",
    "    df_table[f\"image_{model_type}\"] = list(df_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=ind_category)\n",
    "df_table['encoded_label'] = list(mlb.fit_transform(df_table['supercategories']))\n",
    "encoded_df = pd.DataFrame(df_table['encoded_label'].tolist(), columns=ind_category)\n",
    "df_table = pd.concat([df_table, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "#if data_loader.data_source == \"qa\":\n",
    "#    df_table['image_id'] = df_table['image_id'].apply(lambda x: f\"COCO_train2014_{int(x):012d}\")\n",
    "\n",
    "categories_clip = {}\n",
    "for categories in ind_category:\n",
    "    text = 'Category ' + categories\n",
    "    text_tokens = clip.tokenize([text]).to(device)  \n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(text_tokens).flatten().cpu().numpy()\n",
    "        categories_clip[categories] = text_features\n",
    "\n",
    "def calculate_similarity_score(row, type = \"image\"):\n",
    "    if type == \"image\":\n",
    "        column = 'image_clip'\n",
    "    elif type == \"dialogue\":\n",
    "        column = 'dialogue_clip'\n",
    "\n",
    "    cosine_sim = 0\n",
    "    cosine_sim_max = 0\n",
    "    for categories in ind_category:\n",
    "        text_features = categories_clip[categories]\n",
    "        cosine_sim_current = np.dot(text_features, row[column]) / (np.linalg.norm(text_features) * np.linalg.norm(row[column]))\n",
    "        cosine_sim += cosine_sim_current\n",
    "        cosine_sim_max = max(cosine_sim_max, cosine_sim_current)\n",
    "\n",
    "    return cosine_sim, cosine_sim_max\n",
    "\n",
    "df_table['image_score'], df_table['image_score_max'] = zip(*df_table.progress_apply(calculate_similarity_score, axis=1))\n",
    "#dialogue_df_segment['dialogue_score'], dialogue_df_segment['dialogue_score_max'] = zip(*dialogue_df_segment.progress_apply(calculate_similarity_score, type = \"dialogue\", axis=1))\n",
    "df_table['dialogue_score'], df_table['dialogue_score_max'] = zip(*df_table.progress_apply(calculate_similarity_score, axis=1, args=('dialogue',)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_table['dialogue_score_segment'] = pd.DataFrame(dialogue_df_segment.groupby('index')['dialogue_score'].mean())['dialogue_score'].values\n",
    "#df_table['dialogue_score_segment_max'] = pd.DataFrame(dialogue_df_segment.groupby('index')['dialogue_score_max'].mean())['dialogue_score_max'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_figure\n",
    "import importlib\n",
    "importlib.reload(plot_figure)\n",
    "\n",
    "plot_figure.plot_cosine(df_table, save_fig = False, save_format = 'pdf', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text_similarity(row):\n",
    "    a = row['dialogue_clip']\n",
    "    b = row['image_clip']\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "df_table['image_text_similarity'] = df_table.apply(image_text_similarity, axis=1)\n",
    "df_table['overall_simialrity'] = (df_table['image_score_max'] + df_table['dialogue_score_max'])\n",
    "df_table['overall_simialrity_sum'] = (df_table['image_score'] + df_table['dialogue_score'])\n",
    "df_table['overall_simialrity_transform'] = 4 * df_table['overall_simialrity'] * df_table['image_text_similarity']\n",
    "df_table['overall_simialrity_transform_sum'] = 4 * df_table['overall_simialrity_sum'] * df_table['image_text_similarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_table[df_table['OOD'] == 0]['image_text_similarity'], bins=50, kde=True, label='Out-of-Domain')\n",
    "sns.histplot(df_table[df_table['OOD'] == 1]['image_text_similarity'], bins=50, kde=True, label='In-Domain')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Image Max:', ev.fpr_evaluation(df_table['OOD'].values, df_table['image_score_max'].values, 0.95))\n",
    "print('Image Sum:', ev.fpr_evaluation(df_table['OOD'].values, df_table['image_score'].values, 0.95))\n",
    "print('Dialogue Max:', ev.fpr_evaluation(df_table['OOD'].values, df_table['dialogue_score_max'].values, 0.95))\n",
    "print('Dialogue Sum:', ev.fpr_evaluation(df_table['OOD'].values, df_table['dialogue_score'].values, 0.95))\n",
    "#print('Dialogue Segment Max:', ev.fpr_evaluation(df_table['OOD'].values, -df_table['dialogue_score_segment_max'].values, 0.95))\n",
    "#print('Dialogue Segment Sum:', ev.fpr_evaluation(df_table['OOD'].values, -df_table['dialogue_score_segment'].values, 0.95))\n",
    "print(\"Overall Max:\", ev.fpr_evaluation(df_table['OOD'].values, df_table['overall_simialrity'].values, 0.95))\n",
    "print(\"Overall Sum:\", ev.fpr_evaluation(df_table['OOD'].values, df_table['overall_simialrity_sum'].values, 0.95))\n",
    "print(\"Overall Transform Max:\", ev.fpr_evaluation(df_table['OOD'].values, df_table['overall_simialrity_transform'].values, 0.95))\n",
    "print(\"Overall Transform Sum:\", ev.fpr_evaluation(df_table['OOD'].values, df_table['overall_simialrity_transform_sum'].values, 0.95))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.DNN import model \n",
    "\n",
    "image_model_loader = model.model_loader(logger=logger,\n",
    "                                  num_epochs=6,\n",
    "                                  output_size=len(ind_category),\n",
    "                                  learning_rate=0.001,\n",
    "                                  proportion = 0.8,\n",
    "                                  seed = 20)\n",
    "\n",
    "dialogue_model_loader = model.model_loader(logger=logger,\n",
    "                                    num_epochs=6,\n",
    "                                    output_size=len(ind_category),\n",
    "                                    learning_rate=0.001,\n",
    "                                    seed = 20)\n",
    "\n",
    "(\n",
    "    df_ind_train, \n",
    "    df_test, \n",
    "    X_train_image, \n",
    "    X_test_image, \n",
    "    X_train_dialogue, \n",
    "    X_test_dialogue, \n",
    "    Y_train, \n",
    "    Y_test\n",
    ") = image_model_loader.create_dataset(data_loader, df_table, add_mismatch = False, mismatch_num = 10000)\n",
    "\n",
    "df_test['image_text_similarity'] = df_test.apply(image_text_similarity, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_test[df_test['OOD'] == 1]['image_text_similarity'], kde=True, label='In-Domain', bins=50, stat='density')\n",
    "sns.histplot(df_test[df_test['OOD'] == 0]['image_text_similarity'], kde=True, label='Out-of-Domain', bins=50, stat='density')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model_loader.train_model(X_train_image, Y_train, X_test_image, Y_test, ood_category = '_'.join(ood_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_model_loader.train_model(X_train_dialogue, Y_train, X_test_dialogue, Y_test, ood_category = '_'.join(ood_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type_list = [\"prob\", \"energy\", \"logits\", \"msp\"]\n",
    "\n",
    "for score_type in score_type_list:\n",
    "    image_score_sum, image_score_max = image_model_loader.evaluate_on_test(X_test_image, \n",
    "                                                        Y_test,\n",
    "                                                        score_type=score_type,\\\n",
    "                                                        return_score=True)\n",
    "    \n",
    "    dialogue_score_sum, dialogue_score_max = dialogue_model_loader.evaluate_on_test(X_test_dialogue, \n",
    "                                                        Y_test,\n",
    "                                                        score_type=score_type,\\\n",
    "                                                        return_score=True)\n",
    "    \n",
    "    df_test[f'{score_type}_sum_image'] = image_score_sum\n",
    "    df_test[f'{score_type}_max_image'] = image_score_max\n",
    "    df_test[f'{score_type}_sum_dialogue'] = dialogue_score_sum\n",
    "    df_test[f'{score_type}_max_dialogue'] = dialogue_score_max\n",
    "    df_test[f'{score_type}_max_image_tranform'] = 4 * df_test['image_text_similarity'] * df_test[f'{score_type}_max_image'] \n",
    "    df_test[f'{score_type}_max_dialogue_tranform'] = 4 * df_test['image_text_similarity'] * df_test[f'{score_type}_max_dialogue']\n",
    "    df_test[f'{score_type}_sum_image_tranform'] = 4 * df_test['image_text_similarity'] * df_test[f'{score_type}_sum_image']\n",
    "    df_test[f'{score_type}_sum_dialogue_tranform'] = 4 * df_test['image_text_similarity'] * df_test[f'{score_type}_sum_dialogue']\n",
    "\n",
    "    df_test[f'{score_type}_overall_simialrity_max'] = df_test[f'{score_type}_max_image'] + df_test[f'{score_type}_max_dialogue']\n",
    "    df_test[f'{score_type}_overall_simialrity_max_transform'] =  df_test[f'{score_type}_max_image_tranform'] + df_test[f'{score_type}_max_dialogue_tranform']\n",
    "    if score_type in [\"energy\", \"logits\", \"prob\"]:\n",
    "        df_test[f'{score_type}_overall_simialrity_sum'] = df_test[f'{score_type}_sum_image'] + df_test[f'{score_type}_sum_dialogue']\n",
    "        df_test[f'{score_type}_overall_simialrity_sum_transform'] = df_test[f'{score_type}_sum_image_tranform'] + df_test[f'{score_type}_sum_dialogue_tranform']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(plot_figure)\n",
    "\n",
    "score_type = \"msp\"\n",
    "type = \"max\"\n",
    "mode = \"image\"\n",
    "fpr = 95\n",
    "\n",
    "plot_figure.plot_score_distribution(df_test, score_type, type, mode=\"image\", fpr=fpr)\n",
    "plot_figure.plot_score_distribution(df_test, score_type, type, mode=\"dialogue\", fpr=fpr)\n",
    "plot_figure.plot_score_distribution(df_test, score_type, type, mode=\"overall\", fpr=fpr)\n",
    "plot_figure.plot_score_distribution(df_test, score_type, type, mode=\"overall_transform\", fpr=fpr)\n",
    "#plot_figure.plot_kde_joint_distribution(df_test, score_type, type, mode)\n",
    "plot_figure.plot_rough_distribution(df_test, 'image_text_similarity')\n",
    "plot_figure.plot_rough_distribution(df_test, f'{score_type}_{type}_{mode}')\n",
    "plot_figure.plot_joint_distribution(df_test, score_type, type, mode, id = 1, color = 'blue')\n",
    "plot_figure.plot_joint_distribution(df_test, score_type, type, mode, id = 0, color = 'red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store data\n",
    "metrics = []\n",
    "values = []\n",
    "scores = [] \n",
    "\n",
    "def eval_dict(score):\n",
    "    return {\n",
    "        \"FPR\": lambda x: ev.fpr_evaluation(x['OOD'].values, x[score].values, 0.95),\n",
    "        \"AUROC\": lambda x: ev.auroc_evaluation(x['OOD'].values, x[score].values),\n",
    "        \"AUPR\": lambda x: ev.aupr_evaluation(x['OOD'].values, x[score].values)\n",
    "    }\n",
    "\n",
    "# Define the metrics and corresponding functions\n",
    "metric_functions = {\n",
    "    \"Max Cosine\": {\n",
    "        \"Image\": eval_dict('image_score_max'),\n",
    "        \"Dialogue\": eval_dict('dialogue_score_max'),\n",
    "        \"Overall\": eval_dict('overall_simialrity'),\n",
    "        \"Overall_Transform\": eval_dict('overall_simialrity_transform')\n",
    "    },\n",
    "    \"Sum Cosine\": {\n",
    "        \"Image\": eval_dict('image_score'),\n",
    "        \"Dialogue\": eval_dict('dialogue_score'),\n",
    "        \"Overall\": eval_dict('overall_simialrity_sum'),\n",
    "        \"Overall_Transform\": eval_dict('overall_simialrity_transform_sum')\n",
    "    },\n",
    "    \"Energy Sum\": {\n",
    "        \"Image\": eval_dict('energy_sum_image'),\n",
    "        \"Dialogue\": eval_dict('energy_sum_dialogue'),\n",
    "        \"Overall\": eval_dict('energy_overall_simialrity_sum'),\n",
    "        \"Overall_Transform\": eval_dict('energy_overall_simialrity_sum_transform')\n",
    "    },\n",
    "    \"Energy Max\": {\n",
    "        \"Image\": eval_dict('energy_max_image'),\n",
    "        \"Dialogue\": eval_dict('energy_max_dialogue'),\n",
    "        \"Overall\": eval_dict('energy_overall_simialrity_max'),\n",
    "        \"Overall_Transform\": eval_dict('energy_overall_simialrity_max_transform')\n",
    "    },\n",
    "    \"MSP\": {\n",
    "        \"Image\": eval_dict('msp_max_image'),\n",
    "        \"Dialogue\": eval_dict('msp_max_dialogue'),\n",
    "        \"Overall\": eval_dict('msp_overall_simialrity_max'),\n",
    "        \"Overall_Transform\": eval_dict('msp_overall_simialrity_max_transform')\n",
    "    },\n",
    "    \"Max Prob\": {\n",
    "        \"Image\": eval_dict('mp_max_image'),\n",
    "        \"Dialogue\": eval_dict('mp_max_dialogue'),\n",
    "        \"Overall\": eval_dict('mp_overall_simialrity_max'),\n",
    "        \"Overall_Transform\": eval_dict('mp_overall_simialrity_max_transform')\n",
    "    },\n",
    "    \"Max Logits\": {\n",
    "        \"Image\": eval_dict('logits_max_image'),\n",
    "        \"Dialogue\": eval_dict('logits_max_dialogue'),\n",
    "        \"Overall\": eval_dict('logits_overall_simialrity_max'),\n",
    "        \"Overall_Transform\": eval_dict('logits_overall_simialrity_max_transform')\n",
    "    },\n",
    "    \"Sum Logits\": {\n",
    "        \"Image\": eval_dict('logits_sum_image'),\n",
    "        \"Dialogue\": eval_dict('logits_sum_dialogue'),\n",
    "        \"Overall\": eval_dict('logits_overall_simialrity_sum'),\n",
    "        \"Overall_Transform\": eval_dict('logits_overall_simialrity_sum_transform')\n",
    "    }\n",
    "}\n",
    "\n",
    "# Loop through each metric and calculate values\n",
    "for score, items in metric_functions.items():\n",
    "    scores.extend([score] * len(items) * 3)\n",
    "    for metric, funcs in items.items():\n",
    "        metrics.extend([metric] * len(funcs))\n",
    "        values.extend([func(df_test) for func in funcs.values()])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"Metric\": metrics, \"Value\": values, \"Score\": scores})\n",
    "df['Value'] = df['Value'].apply(lambda x: round(x, 3))\n",
    "#df_grouped = df.groupby('Metric')['Value'].apply(list).reset_index()\n",
    "result = df.groupby(['Metric', 'Score'])['Value'].agg(list).unstack().transpose()\n",
    "result_df = result[['Image', 'Dialogue', 'Overall', 'Overall_Transform']]\n",
    "result_df.reset_index(inplace=True)\n",
    "order = ['Max Cosine', 'Sum Cosine', 'Energy Sum', 'Energy Max', 'MSP', 'Max Prob', 'Max Logits', 'Sum Logits']\n",
    "result_df = result_df.set_index('Score').loc[order].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_percentage(lst):\n",
    "    return ' / '.join(f'{x*100:.1f}' for x in lst)\n",
    "\n",
    "result_df['Image'] = result_df['Image'].apply(convert_to_percentage)\n",
    "result_df['Dialogue'] = result_df['Dialogue'].apply(convert_to_percentage)\n",
    "result_df['Overall'] = result_df['Overall'].apply(convert_to_percentage)\n",
    "result_df['Overall_Transform'] = result_df['Overall_Transform'].apply(convert_to_percentage)\n",
    "\n",
    "latex_table = result_df.to_latex(index=False, column_format='|l|c|c|c|c|', header=[\"Score\", \"Image\", \"Dialogue\", \"Overall\", \"Overall_Transform\"], escape=False)\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_stats(model, train_loader):\n",
    "    model.eval()\n",
    "    class_means = []\n",
    "    class_covariances = []\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in train_loader:\n",
    "            data = data.cuda()\n",
    "            features = model(data)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(target.numpy())\n",
    "    \n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    for label in np.unique(all_labels):\n",
    "        class_features = all_features[all_labels == label]\n",
    "        class_mean = np.mean(class_features, axis=0)\n",
    "        class_cov = np.cov(class_features, rowvar=False)\n",
    "        class_means.append(class_mean)\n",
    "        class_covariances.append(class_cov)\n",
    "    \n",
    "    return class_means, class_covariances\n",
    "\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    x_minus_mean = x - mean\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    left_term = np.dot(x_minus_mean, inv_cov)\n",
    "    mahalanobis_dist = np.sqrt(np.dot(left_term, x_minus_mean.T))\n",
    "    return mahalanobis_dist\n",
    "\n",
    "def calculate_ood_scores(model, test_loader, class_means, class_covariances):\n",
    "    model.eval()\n",
    "    ood_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            features = model(data).cpu().numpy()\n",
    "            \n",
    "            for feature in features:\n",
    "                min_distance = float('inf')\n",
    "                for mean, cov in zip(class_means, class_covariances):\n",
    "                    distance = mahalanobis_distance(feature, mean, cov)\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                ood_scores.append(min_distance)\n",
    "    \n",
    "    return np.array(ood_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results on OOD\n",
    "\n",
    "# Without Mismatching Pair, Test ID instance 23233, Test OOD instance 6051\n",
    "\n",
    "\n",
    "## Without image text similarity score\n",
    "| Score      | Image                 | Dialogue              | Overall               |\n",
    "|:-----------|:----------------------|:----------------------|:----------------------|\n",
    "| Energy Max | [0.373, 0.937, 0.983] | [0.791, 0.886, 0.972] | [0.32, 0.952, 0.988]  |\n",
    "| Energy Sum | [0.236, 0.964, 0.991] | [0.508, 0.935, 0.984] | [0.151, 0.974, 0.994] |\n",
    "| MSP        | [0.894, 0.625, 0.873] | [0.919, 0.68, 0.905]  | [0.885, 0.657, 0.889] |\n",
    "| Max Cosine | [0.877, 0.711, 0.905] | [0.971, 0.54, 0.835]  | [0.964, 0.579, 0.857] |\n",
    "| Max Logits | [0.373, 0.937, 0.983] | [0.791, 0.886, 0.972] | [0.331, 0.951, 0.988] |\n",
    "| Max Prob   | [0.373, 0.937, 0.983] | [0.791, 0.886, 0.972] | [0.507, 0.927, 0.982] |\n",
    "| Sum Cosine | [0.94, 0.576, 0.84]   | [0.978, 0.494, 0.814] | [0.972, 0.51, 0.817]  |\n",
    "| Sum Logits | [0.904, 0.762, 0.934] | [0.974, 0.758, 0.931] | [0.932, 0.788, 0.943] |\n",
    "\n",
    "## With image text similarity score overall\n",
    "| Score      | Image                 | Dialogue              | Overall               |\n",
    "|:-----------|:----------------------|:----------------------|:----------------------|\n",
    "| Energy Max | [0.373, 0.937, 0.983] | [0.791, 0.886, 0.972] | [0.346, 0.945, 0.986] |\n",
    "| Energy Sum | [0.236, 0.964, 0.991] | [0.508, 0.935, 0.984] | [0.194, 0.97, 0.993]  |\n",
    "| MSP        | [0.894, 0.625, 0.873] | [0.919, 0.68, 0.905]  | [0.908, 0.574, 0.846] |\n",
    "| Max Cosine | [0.877, 0.711, 0.905] | [0.971, 0.54, 0.835]  | [0.964, 0.579, 0.857] |\n",
    "| Max Logits | [0.373, 0.937, 0.983] | [0.791, 0.886, 0.972] | [0.35, 0.945, 0.986]  |\n",
    "| Max Prob   | [0.373, 0.937, 0.983] | [0.791, 0.886, 0.972] | [0.731, 0.793, 0.934] |\n",
    "| Sum Cosine | [0.94, 0.576, 0.84]   | [0.978, 0.494, 0.814] | [0.972, 0.51, 0.817]  |\n",
    "| Sum Logits | [0.904, 0.762, 0.934] | [0.974, 0.758, 0.931] | [0.955, 0.764, 0.934] |\n",
    "\n",
    "# With 5000 mismatching pairs, testing ID instance 23233, test OOD isntance 11051\n",
    "\n",
    "\n",
    "## Without image text similarity\n",
    "| Score      | Image                 | Dialogue              | Overall               |\n",
    "|:-----------|:----------------------|:----------------------|:----------------------|\n",
    "| Energy Max | [0.625, 0.747, 0.815] | [0.859, 0.721, 0.81]  | [0.614, 0.758, 0.838] |\n",
    "| Energy Sum | [0.546, 0.762, 0.821] | [0.698, 0.748, 0.819] | [0.522, 0.767, 0.843] |\n",
    "| MSP        | [0.92, 0.57, 0.737]   | [0.932, 0.604, 0.762] | [0.921, 0.598, 0.773] |\n",
    "| Max Cosine | [0.908, 0.618, 0.758] | [0.961, 0.523, 0.708] | [0.959, 0.544, 0.724] |\n",
    "| Max Logits | [0.625, 0.747, 0.815] | [0.859, 0.721, 0.81]  | [0.619, 0.758, 0.839] |\n",
    "| Max Prob   | [0.625, 0.747, 0.815] | [0.859, 0.721, 0.81]  | [0.707, 0.765, 0.848] |\n",
    "| Sum Cosine | [0.945, 0.543, 0.712] | [0.965, 0.497, 0.693] | [0.963, 0.506, 0.695] |\n",
    "| Sum Logits | [0.923, 0.65, 0.783]  | [0.963, 0.648, 0.782] | [0.953, 0.672, 0.814] |\n",
    "\n",
    "## With imge text similarity score on overall \n",
    "| Score      | Image                 | Dialogue              | Overall               |\n",
    "|:-----------|:----------------------|:----------------------|:----------------------|\n",
    "| Energy Max | [0.625, 0.747, 0.815] | [0.859, 0.721, 0.81]  | [0.598, 0.862, 0.932] |\n",
    "| Energy Sum | [0.546, 0.762, 0.821] | [0.698, 0.748, 0.819] | [0.511, 0.872, 0.934] |\n",
    "| MSP        | [0.92, 0.57, 0.737]   | [0.932, 0.604, 0.762] | [0.752, 0.723, 0.831] |\n",
    "| Max Cosine | [0.908, 0.618, 0.758] | [0.961, 0.523, 0.708] | [0.959, 0.544, 0.724] |\n",
    "| Max Logits | [0.625, 0.747, 0.815] | [0.859, 0.721, 0.81]  | [0.607, 0.86, 0.931]  |\n",
    "| Max Prob   | [0.625, 0.747, 0.815] | [0.859, 0.721, 0.81]  | [0.569, 0.859, 0.924] |\n",
    "| Sum Cosine | [0.945, 0.543, 0.712] | [0.965, 0.497, 0.693] | [0.963, 0.506, 0.695] |\n",
    "| Sum Logits | [0.923, 0.65, 0.783]  | [0.963, 0.648, 0.782] | [0.975, 0.546, 0.709] |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
